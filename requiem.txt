class NeuralNetworkClassifier():

    def __call__(self):
        return self.name

    def __init__(self, name, hiddenLayers, inputVariables):
        self.name           = name
        self.hiddenLayers   = hiddenLayers

        self.FullData       = pd.read_csv(f"{dataPath}/dataLearn/FullDataClassified.csv", index_col=0)
        self.signalData     = pd.read_csv(f"{dataPath}/dataLearn/SignalData.csv",index_col=0)
        self.backgroundData = pd.read_csv(f"{dataPath}/dataLearn/BackgroundData.csv",index_col=0)
        train               = pd.read_csv(f"{dataPath}/dataLearn/TrainingData.csv",index_col=0)
        test                = pd.read_csv(f"{dataPath}/dataLearn/TestingData.csv",index_col=0)

        self.X_train        = train[inputVariables]
        self.Y_train        = train["isSignal"]
        self.W_train        = train["weights"]

        self.X_test         = test[inputVariables]
        self.Y_test         = test["isSignal"]
        self.W_test         = test["weights"]
        
        self.createModel()
        self.trainModel()
    
    def createModel(self):
        self.NNmodel = Sequential()
        for layer in self.hiddenLayers: self.NNmodel.add(layer)
        self.NNmodel.compile(loss="binary_crossentropy", optimizer="adam", weighted_metrics=["accuracy"])

    def trainModel(self):
        EarlyStoppingCallback = EarlyStopping(monitor="val_loss", patience=10)
        history = self.NNmodel.fit(self.X_train, self.Y_train, sample_weight=self.W_train, validation_data=(self.X_test, self.Y_test), batch_size=20, epochs=500, callbacks=[EarlyStoppingCallback])
        return history
    
    def evaluateModel(self):
        Y_predTest  = self.NNmodel.predict(self.X_test)
        Y_predTrain = self.NNmodel.predict(self.X_train)

        NNfalsePositiveRate, NNtruePositiveRate, threshold = roc_curve(self.Y_test, Y_predTest)
        confusionMatrixNN = confusion_matrix(y_true=self.Y_test, y_pred=np.round(Y_predTest))
        
        print(f"\nConfusion Matrix \n{confusionMatrixNN}\n")
        print(f"Test Data Score     : {roc_auc_score(self.Y_test,  Y_predTest)}\n")
        print(f"Training Data Score : {roc_auc_score(self.Y_train, Y_predTrain)}\n")
        return (NNfalsePositiveRate, NNtruePositiveRate, threshold)
    
    def summary(self):
        return self.NNmodel.summary()

    def predict(self, inputData):
        return self.NNmodel.predict(inputData).flatten()

    def executeAll(self):
        self.summary()
        self.TrainModel()
        self.EvaluateModel()
        self.saveModel()

    def saveModel(self, filename):
        os.makedirs(os.path.dirname(f"{modelPath}/{name}"), exist_ok=True)
        joblib.dump(self, f"{modelPath}/{name}/{filename}.joblib")



#########

    hiddenLayers   =   [Dense(len(inputVariables),    input_shape = (len(inputVariables),),    activation="relu"),
                        Dropout(0.025),
                        Dense(len(inputVariables)*10, input_shape = (len(inputVariables),),    activation="relu"),
                        Dropout(0.025),
                        Dense(len(inputVariables),    input_shape = (len(inputVariables)*10,), activation="relu"),
                        Dropout(0.025),
                        Dense(1,                      input_shape = (len(inputVariables),),    activation="sigmoid")]
    

##########

class CombinedClassifier():

    def __call__(self):
        pass

    def __init__(self, name, inputVariables, trainData, testData, models):

        self.name           = name
        self.X_train        = trainData[inputVariables]
        self.Y_train        = trainData["isSignal"]
        self.W_train        = trainData["weights"]

        self.X_test         = testData[inputVariables]
        self.Y_test         = testData["isSignal"]
        self.W_test         = testData["weights"]

        self.allModels = models
    
    def trainModel(self):
        for model in self.allModels:
            model.trainModel()

    def predict(self, inputData):
        predictions = np.array([model.predict(inputData) for model in self.allModels])
        meanPredictions = np.mean(predictions, axis=0)
        return meanPredictions

    def evaluateModel(self):
        self.summaryString = ""
        Y_predTest  = self.predict(self.X_test)
        Y_predTrain = self.predict(self.X_train)

        confusionMatrix = confusion_matrix(y_true=self.Y_test, y_pred=np.round(Y_predTest))
        self.summaryString += f"\nCombined Classifier\n"
        self.summaryString += f"Confusion Matrix\n{confusionMatrix}\n"
        self.summaryString += f"Test Data Score     : {roc_auc_score(self.Y_test,  Y_predTest)}\n"
        self.summaryString += f"Training Data Score : {roc_auc_score(self.Y_train, Y_predTrain)}\n"
        
        return self.summaryString
    
    def createROCcurve(self):
        Y_predTest = self.predict(self.X_test).flatten()
        falsePositiveRate, truePositiveRate, _ = roc_curve(self.Y_test, Y_predTest)
        return (falsePositiveRate, truePositiveRate)
    
    def saveModel(self):
        os.makedirs(os.path.dirname(f"savedModels/{self.name}/"), exist_ok=True)

        summaryFile = open(f"savedModels/{self.name}/COsummary.txt", "w")
        summaryFile.write(self.summaryString) 
        summaryFile.close()

        joblib.dump(self, f"savedModels/{name}/CO{name}.joblib")

    def createInFull(self):
        print("Evaluating...\n")
        self.evaluateModel()
        print("Saving...\n")
        self.saveModel()
        print("Done!\n")

########


import os
import joblib
import pandas as pd

from DataFormatting             import splitSignalEvenly
from sklearn.metrics            import confusion_matrix, roc_curve, roc_auc_score
from sklearn.preprocessing      import MinMaxScaler, normalize
from misc                       import *
from tensorflow.keras.models    import Sequential     # type: ignore
from tensorflow.keras.layers    import Dense, Dropout, BatchNormalization # type: ignore
from tensorflow.keras.callbacks import EarlyStopping  # type: ignore
from sklearn.ensemble           import GradientBoostingClassifier, AdaBoostClassifier, RandomForestClassifier


class BaseClassifier():

    # stuff thats common

    # attributes
    # name
    # inputVariables
    # X/Y/W data

    # methods
    # evaluate model
    # create ROC curve (with cut)
    # save model
    # create in full
    # find figure of merit 
    # predict figure of merit
    # create probability distribution with cuts
    # create variable dsitibution with cut
    pass


class NeuralNetworkClassifier():

    def __call__(self):
        return (self.name, self.model)

    def __init__(self, name, hiddenLayers, inputVariables, trainData, testData, batchSize):
        self.name           = name
        self.hiddenLayers   = hiddenLayers
        self.inputVariables = inputVariables
        self.batchSize      = batchSize

        self.Normaliser     = MinMaxScaler()
        ScaledTrainData     = self.Normaliser.fit_transform(trainData[inputVariables])
        ScaledTestData      = self.Normaliser.fit_transform( testData[inputVariables])
        
        self.X_train        = pd.DataFrame(ScaledTrainData, columns=inputVariables)
        self.Y_train        = trainData["isSignal"]
        self.W_train        = trainData["weights"]

        self.X_test         = pd.DataFrame(ScaledTestData,  columns=inputVariables)
        self.Y_test         = testData["isSignal"]
        self.W_test         = testData["weights"]

        self.model = Sequential(self.hiddenLayers)
        self.model.compile(loss="binary_crossentropy", optimizer="adam", weighted_metrics=["accuracy"])

    def trainModel(self):
        EarlyStoppingCallback = EarlyStopping(monitor="val_loss", patience=10)
        self.history = self.model.fit(self.X_train, self.Y_train, sample_weight=self.W_train, validation_data=(self.X_test, self.Y_test), batch_size=self.batchSize, epochs=500, callbacks=[EarlyStoppingCallback])

    def predict(self, inputData):
        transformedData = self.Normaliser.fit_transform(inputData[self.inputVariables])
        return self.model.predict(transformedData).flatten()
    
    def evaluateModel(self, inputData=None):
        self.summaryString   = ""
        Y_predTest           = self.predict(self.X_test)
        Y_predTrain          = self.predict(self.X_train)

        confusionMatrixTest  = confusion_matrix(y_true=self.Y_test,  y_pred=roundUp(Y_predTest))
        confusionMatrixTrain = confusion_matrix(y_true=self.Y_train, y_pred=roundUp(Y_predTrain))

        self.summaryString  += f"\n{NNSummaryToSring(self.model)}\n"
        self.summaryString  += "\n"
        self.summaryString  += f"Test Data Score     : {roc_auc_score(self.Y_test,  Y_predTest)}\n"
        self.summaryString  += f"Test Confusion Matrix\n{confusionMatrixTest}\n"
        self.summaryString  += "\n"
        self.summaryString  += f"Training Data Score : {roc_auc_score(self.Y_train, Y_predTrain)}\n"
        self.summaryString  += f"Train Confusion Matrix\n{confusionMatrixTrain}\n"


        if inputData != None:
            Y_predInput          = self.predict(inputData)
            confusionMatrixInput = confusion_matrix(y_true=self.Y_test, y_pred=roundUp(Y_predInput))
            self.summaryString  += "\n"
            self.summaryString  += f"Input Data Score    : {roc_auc_score(inputData["isSignal"], Y_predInput)}\n"
            self.summaryString  += f"Input Confusion Matrix\n{confusionMatrixInput}\n"


        self.summaryString += f"\n"
        self.summaryString += f"Input Variables     : {self.inputVariables}\n"

    def createROCcurve(self, inputData):
        Y_predTest = self.predict(inputData)
        falsePositiveRate, truePositiveRate, _ = roc_curve(y_true=inputData["isSignal"], y_score=Y_predTest)
        return (falsePositiveRate, truePositiveRate)

    def saveModel(self):
        os.makedirs(os.path.dirname(f"savedModels/{self.name}/"), exist_ok=True)
        summaryFile = open(f"savedModels/{self.name}/NNsummary.txt", "w")
        summaryFile.write(self.summaryString) 
        summaryFile.close()
        joblib.dump(self, f"savedModels/{self.name}/NN{self.name}.joblib")

    def createInFull(self,inputData=None):
        print("\nTraining...\n")
        self.trainModel()
        print("Evaluating...\n")
        self.evaluateModel(inputData)
        print("Saving...\n")
        self.saveModel()
        print("Done!\n")


class ForestClassifiers():

    def __call__(self):
        return self.name

    def __init__(self, name, model, inputVariables, trainData, testData):
        self.name           = name
        self.model          = model
        self.inputVariables = inputVariables

        self.X_train        = trainData[inputVariables]
        self.Y_train        = trainData["isSignal"]
        self.W_train        = trainData["weights"]

        self.X_test         = testData[inputVariables]
        self.Y_test         = testData["isSignal"]
        self.W_test         = testData["weights"]

    def trainModel(self):
        fittedEstimator = self.model.fit(self.X_train, self.Y_train, sample_weight=self.W_train)
        return fittedEstimator
    
    def predict(self, inputData):
        return self.model.predict_proba(inputData)[:, 1]
    
    def evaluateModel(self, inputData=None):
        self.summaryString   = ""
        Y_predTest           = self.predict(self.X_test)
        Y_predTrain          = self.predict(self.X_train)

        confusionMatrixTest  = confusion_matrix(y_true=self.Y_test,  y_pred=roundUp(Y_predTest))
        confusionMatrixTrain = confusion_matrix(y_true=self.Y_train, y_pred=roundUp(Y_predTrain))

        self.summaryString  += "\n"
        self.summaryString  += f"Test Data Score     : {roc_auc_score(self.Y_test,  Y_predTest)}\n"
        self.summaryString  += f"Test Confusion Matrix\n{confusionMatrixTest}\n"
        self.summaryString  += "\n"
        self.summaryString  += f"Training Data Score : {roc_auc_score(self.Y_train, Y_predTrain)}\n"
        self.summaryString  += f"Train Confusion Matrix\n{confusionMatrixTrain}\n"

        if inputData != None:
            Y_predInput          = self.predict(inputData)
            confusionMatrixInput = confusion_matrix(y_true=self.Y_test, y_pred=roundUp(Y_predInput))
            self.summaryString  += "\n"
            self.summaryString  += f"Input Data Score    : {roc_auc_score(inputData["isSignal"], Y_predInput)}\n"
            self.summaryString  += f"Input Confusion Matrix\n{confusionMatrixInput}\n"

        self.summaryString += f"\n"
        self.summaryString += f"Input Variables     : {self.inputVariables}\n"

    def createROCcurve(self):
        AFfalsePositiveRate, AFtruePositiveRate, _ = createTreeROCcurve(self.model, self.X_test, self.Y_test)
        return (AFfalsePositiveRate, AFtruePositiveRate)

    def saveModel(self):
        if   type(self.model) == GradientBoostingClassifier: abrev = "GB"
        elif type(self.model) == AdaBoostClassifier:         abrev = "AD"
        elif type(self.model) == RandomForestClassifier:     abrev = "RF"
        else :                                               abrev = "**"

        os.makedirs(os.path.dirname(f"savedModels/{self.name}/"), exist_ok=True)
        summaryFile = open(f"savedModels/{self.name}/{abrev}summary.txt", "w")
        summaryFile.write(self.summaryString) 
        summaryFile.close()

        joblib.dump(self, f"{modelPath}/{self.name}/{abrev}{self.name}.joblib")

    def createInFull(self):
        print("\nTraining...\n")
        self.trainModel()
        print("Evaluating...\n")
        self.evaluateModel()
        print("Saving...\n")
        self.saveModel()
        print("Done!\n")


def createAllModels(name = "test"):

    inputVariables  = ["nTracks", "B_P", "B_Cone3_B_ptasy", "B_ETA", "B_MINIPCHI2", "B_SmallestDeltaChi2OneTrack", "B_FD_OWNPV", "piminus_PT", "piminus_IP_OWNPV","daughter_neutral_PT", "daughter_neutral_IP_OWNPV", "daughterplus_PT","daughterplus_IP_OWNPV"]

    testData        = pd.read_csv("data/Raw/TestData.csv")
    trainData       = pd.read_csv("data/Raw/TrainData.csv")

    hiddenLayers    =  [BatchNormalization(axis = 1),
                        Dense(len(inputVariables),    input_shape = (len(inputVariables),),    activation="relu",    kernel_initializer="random_normal"),
                        Dropout(0.05),
                        BatchNormalization(axis = 1),
                        Dense(len(inputVariables)*12, input_shape = (len(inputVariables),),    activation="relu",    kernel_initializer="random_normal"),
                        Dropout(0.05),
                        BatchNormalization(axis = 1),
                        Dense(len(inputVariables),    input_shape = (len(inputVariables)*12,), activation="relu",    kernel_initializer="random_normal"),
                        Dropout(0.05),
                        BatchNormalization(axis = 1),
                        Dense(1,                      input_shape = (len(inputVariables),),    activation="sigmoid", kernel_initializer="random_normal")]
    
    modelParams = {"NN" : hiddenLayers, 
                   "GB" : GradientBoostingClassifier(n_estimators=800, learning_rate=0.012, max_depth=4),
                   "AD" : AdaBoostClassifier(n_estimators=500, learning_rate=0.1),
                   "RF" : RandomForestClassifier(n_estimators=50, max_depth=3)}
    
    print("- Neural Network")
    NN = NeuralNetworkClassifier(name, modelParams["NN"], inputVariables, trainData, testData)
    NN.createInFull()

    print("- Random Forest")
    RF = ForestClassifiers(name,  modelParams["RF"], inputVariables, trainData, testData)
    RF.createInFull()

    print("- Gradient Boost")
    GB = ForestClassifiers(name, modelParams["GB"], inputVariables, trainData, testData)
    GB.createInFull()

    print("- Adaboost")
    AD = ForestClassifiers(name,  modelParams["AD"], inputVariables, trainData, testData)
    AD.createInFull()


if __name__ == "__main__":
    pass


####


# class ForestClassifiers():

#     def __call__(self):
#         return self.name

#     def __init__(self, name, model, inputVariables, trainData, testData):
#         self.name           = name
#         self.model          = model
#         self.inputVariables = inputVariables

#         self.X_train        = trainData[inputVariables]
#         self.Y_train        = trainData["isSignal"]
#         self.W_train        = trainData["weights"]

#         self.X_test         = testData[inputVariables]
#         self.Y_test         = testData["isSignal"]
#         self.W_test         = testData["weights"]

#     def trainModel(self):
#         fittedEstimator = self.model.fit(self.X_train, self.Y_train, sample_weight=self.W_train)
#         return fittedEstimator
    
#     def predict(self, inputData):
#         return self.model.predict_proba(inputData)[:, 1]
    
#     def evaluateModel(self, inputData=None):
#         self.summaryString   = ""
#         Y_predTest           = self.predict(self.X_test)
#         Y_predTrain          = self.predict(self.X_train)

#         confusionMatrixTest  = confusion_matrix(y_true=self.Y_test,  y_pred=roundUp(Y_predTest))
#         confusionMatrixTrain = confusion_matrix(y_true=self.Y_train, y_pred=roundUp(Y_predTrain))

#         self.summaryString  += "\n"
#         self.summaryString  += f"Test Data Score     : {roc_auc_score(self.Y_test,  Y_predTest)}\n"
#         self.summaryString  += f"Test Confusion Matrix\n{confusionMatrixTest}\n"
#         self.summaryString  += "\n"
#         self.summaryString  += f"Training Data Score : {roc_auc_score(self.Y_train, Y_predTrain)}\n"
#         self.summaryString  += f"Train Confusion Matrix\n{confusionMatrixTrain}\n"

#         if inputData != None:
#             Y_predInput          = self.predict(inputData)
#             confusionMatrixInput = confusion_matrix(y_true=self.Y_test, y_pred=roundUp(Y_predInput))
#             self.summaryString  += "\n"
#             self.summaryString  += f"Input Data Score    : {roc_auc_score(inputData["isSignal"], Y_predInput)}\n"
#             self.summaryString  += f"Input Confusion Matrix\n{confusionMatrixInput}\n"

#         self.summaryString += f"\n"
#         self.summaryString += f"Input Variables     : {self.inputVariables}\n"

#     def createROCcurve(self):
#         AFfalsePositiveRate, AFtruePositiveRate, _ = createTreeROCcurve(self.model, self.X_test, self.Y_test)
#         return (AFfalsePositiveRate, AFtruePositiveRate)

#     def saveModel(self):
#         if   type(self.model) == GradientBoostingClassifier: abrev = "GB"
#         elif type(self.model) == AdaBoostClassifier:         abrev = "AD"
#         elif type(self.model) == RandomForestClassifier:     abrev = "RF"
#         else :                                               abrev = "**"

#         os.makedirs(os.path.dirname(f"savedModels/{self.name}/"), exist_ok=True)
#         summaryFile = open(f"savedModels/{self.name}/{abrev}summary.txt", "w")
#         summaryFile.write(self.summaryString) 
#         summaryFile.close()

#         joblib.dump(self, f"{modelPath}/{self.name}/{abrev}{self.name}.joblib")

#     def createInFull(self):
#         print("\nTraining...\n")
#         self.trainModel()
#         print("Evaluating...\n")
#         self.evaluateModel()
#         print("Saving...\n")
#         self.saveModel()
#         print("Done!\n")


# def createAllModels(name = "test"):

#     inputVariables  = ["nTracks", "B_P", "B_Cone3_B_ptasy", "B_ETA", "B_MINIPCHI2", "B_SmallestDeltaChi2OneTrack", "B_FD_OWNPV", "piminus_PT", "piminus_IP_OWNPV","daughter_neutral_PT", "daughter_neutral_IP_OWNPV", "daughterplus_PT","daughterplus_IP_OWNPV"]

#     testData        = pd.read_csv("data/Raw/TestData.csv")
#     trainData       = pd.read_csv("data/Raw/TrainData.csv")

#     hiddenLayers    =  [BatchNormalization(axis = 1),
#                         Dense(len(inputVariables),    input_shape = (len(inputVariables),),    activation="relu",    kernel_initializer="random_normal"),
#                         Dropout(0.05),
#                         BatchNormalization(axis = 1),
#                         Dense(len(inputVariables)*12, input_shape = (len(inputVariables),),    activation="relu",    kernel_initializer="random_normal"),
#                         Dropout(0.05),
#                         BatchNormalization(axis = 1),
#                         Dense(len(inputVariables),    input_shape = (len(inputVariables)*12,), activation="relu",    kernel_initializer="random_normal"),
#                         Dropout(0.05),
#                         BatchNormalization(axis = 1),
#                         Dense(1,                      input_shape = (len(inputVariables),),    activation="sigmoid", kernel_initializer="random_normal")]
    
#     modelParams = {"NN" : hiddenLayers, 
#                    "GB" : GradientBoostingClassifier(n_estimators=800, learning_rate=0.012, max_depth=4),
#                    "AD" : AdaBoostClassifier(n_estimators=500, learning_rate=0.1),
#                    "RF" : RandomForestClassifier(n_estimators=50, max_depth=3)}
    
#     print("- Neural Network")
#     NN = NeuralNetworkClassifier(name, modelParams["NN"], inputVariables, trainData, testData)
#     NN.createInFull()

#     print("- Random Forest")
#     RF = ForestClassifiers(name,  modelParams["RF"], inputVariables, trainData, testData)
#     RF.createInFull()

#     print("- Gradient Boost")
#     GB = ForestClassifiers(name, modelParams["GB"], inputVariables, trainData, testData)
#     GB.createInFull()

#     print("- Adaboost")
#     AD = ForestClassifiers(name,  modelParams["AD"], inputVariables, trainData, testData)
#     AD.createInFull()



########

    def findFigureOfMerit(self, inputData = None):
        
        if inputData is None: 
            inputData = self.X_test
            inputData["isSignal"] = self.Y_test

        FigureOfMerit = []
        inputData["Prediction"] = self.predict(inputData)
        SignalData = inputData[inputData["isSignal"] == 1.0]
        BackgroundData = inputData[inputData["isSignal"] == 0.0]
    
        for cut in np.linspace(0,1,1001):

            SigEff  = len(SignalData[ (SignalData["Prediction"] > cut) ]) / len(SignalData)
            BacEff  = len(BackgroundData[ (BackgroundData["Prediction"] > cut) ]) / len(BackgroundData)
            Bcomb = len(inputData[  (inputData["Prediction"]  > cut) ])
            FOM   = len(SignalData)*SigEff / np.sqrt(len(SignalData)*SigEff + Bcomb)

            FigureOfMerit.append([cut, SigEff, BacEff, FOM])

        return pd.DataFrame(np.array(FigureOfMerit), columns=["cut", "signalEff", "backgroundEff", "FOM"])

    def updateBestCut(self, inputData = None):
        FOMData  = self.findFigureOfMerit(inputData)
        cutIndex = FOMData["FOM"].idxmax()
        self.cut = list(FOMData["cut"])[cutIndex]     

    def plotFigureOfMerit(self, inputData = None):

        FOMData = self.findFigureOfMerit(inputData)

        fig, ax1 = plt.subplots()
        ax2 = ax1.twinx()

        ax1.set_title(f"{self.name}")
        ax1.plot(FOMData["cut"], FOMData["FOM"], c = colors["red"],  label="Figure of Merit")
        ax2.plot(FOMData["cut"], FOMData["signalEff"], c = colors["blue"], label="Signal Efficiency")
        ax2.plot(FOMData["cut"], FOMData["backgroundEff"], c = colors["orange"], label="Background Efficiency")

        ax1.set_xlabel("Cut")
        ax1.set_ylabel("Figure of Merit"  , color=colors["red"])
        ax2.set_ylabel("Signal Efficiency", color=colors["blue"])

        ax1.set_xlim(0,1)
        ax1.set_ylim(0)
        ax2.set_ylim(0)

        plt.show()

    def plotProbabilityDistribution(self, inputData = None):

        if inputData is None: 
            inputData = self.X_test
            inputData["isSignal"] = self.Y_test
            inputData["weights"]  = self.W_test
            
        SignalData     = inputData[inputData["isSignal"] == 1.0]
        BackgroundData = inputData[inputData["isSignal"] == 0.0]
        SignalData["Prediction"]     = self.predict(SignalData)
        BackgroundData["Prediction"] = self.predict(BackgroundData)

        plt.title("Model Prediction Distribution")
        plt.hist( [SignalData["Prediction"], BackgroundData["Prediction"]],
        weights = [SignalData["weights"],    BackgroundData["weights"]   ],
        label   = ["Signal",                 "Background"],
        **histStyle)
        plt.legend()
        plt.show()

    def plotVariableDistribution(self, variable, inputData = None):
        
        #plot doesnt look right because of transformation?
        if inputData is None: 
            inputData = self.X_test
            inputData["isSignal"] = self.Y_test
            inputData["weights"]  = self.W_test

        SignalData     = inputData[inputData["isSignal"] == 1.0]
        BackgroundData = inputData[inputData["isSignal"] == 0.0]

        plt.title(f"{variable} Prediction Distribution")
        plt.hist( [SignalData[variable], BackgroundData[variable] ],
        weights = [SignalData["weights"],BackgroundData["weights"]],
        label   = ["Signal",            "Background"],
        **histStyle)
        plt.legend()
        plt.show()

########

# stuff thats common

# attributes
# name
# inputVariables
# X/Y/W data

# methods
# evaluate model
# create ROC curve (with cut)
# save model
# create in full
# find figure of merit 
# predict figure of merit
# create probability distribution with cuts
# create variable dsitibution with cut



#####



    plt.plot([0,1],[0,1], c="black", linestyle=":")
    plt.grid(linestyle="--",alpha=0.3)
    plt.title("ROC curves")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Postive Rate")

    (TPR, FPR) = NN.getROCCurveValues()
    plt.plot(TPR, FPR)
    plt.show()
 

    (SignalData, BackgroundData) = NN.getSeperatedPredictions()

    plt.hist(
    [BackgroundData["NNBestpipi Prediction"],SignalData["NNBestpipi Prediction"]],
    label = ["b","s"], 
    bins  = 50, 
    histtype = "step")

    plt.legend()
    plt.show()

###

best modelParams

    hiddenLayers    =  [Dense(len(inputVariables),    input_shape = (len(inputVariables),),    activation="relu", kernel_initializer="random_normal"),
                        Dropout(0.2),
                        Dense(len(inputVariables)*12, input_shape = (len(inputVariables),),    activation="relu", kernel_initializer="random_normal"),
                        Dropout(0.2),
                        Dense(len(inputVariables),    input_shape = (len(inputVariables)*12,), activation="relu", kernel_initializer="random_normal"),
                        Dropout(0.2),
                        Dense(1,                      input_shape = (len(inputVariables),),    activation="sigmoid",    kernel_initializer="random_normal")]
    
    print("- Neural Network")
    NN = NeuralNetworkClassifier(hiddenLayers, **GlobalParams)
    NN.createInFull()

    # print("- Random Forest")
    # RF = ForestClassifier(RandomForestClassifier(n_estimators=1500, max_depth=6, verbose=1), **GlobalParams)
    # RF.createInFull()
    
    # print("- AdaBoost")
    # AD = ForestClassifier(AdaBoostClassifier(n_estimators=500, learning_rate=0.1), **GlobalParams)
    # AD.createInFull()

    # print("- Gradient Boost")
    # GB = ForestClassifier(GradientBoostingClassifier(n_estimators=800, learning_rate=0.012, max_depth=4, verbose=1), **GlobalParams)
    # GB.createInFull()

test modelParams


    hiddenLayers    =  [Dense(1,                      input_shape = (len(inputVariables),),    activation="sigmoid",    kernel_initializer="random_normal")]
    
    print("- Neural Network")
    NN = NeuralNetworkClassifier(hiddenLayers, **GlobalParams)
    NN.createInFull()

    # print("- Random Forest")
    # RF = ForestClassifier(RandomForestClassifier(n_estimators=50, max_depth=3, verbose=1), **GlobalParams)
    # RF.createInFull()
    
    # print("- AdaBoost")
    # AD = ForestClassifier(AdaBoostClassifier(n_estimators=50, learning_rate=0.1), **GlobalParams)
    # AD.createInFull()

    # print("- Gradient Boost")
    # GB = ForestClassifier(GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, max_depth=3, verbose=1), **GlobalParams)
    # GB.createInFull()


def createTestModel(mode = "pipi"):
    name            = f"test{mode}"
    inputVariables  = ["nTracks", "B_P", "B_Cone3_B_ptasy", "B_ETA", "B_MINIPCHI2", "B_SmallestDeltaChi2OneTrack", "B_FD_OWNPV", "piminus_PT", "piminus_IP_OWNPV","daughter_neutral_PT", "daughter_neutral_IP_OWNPV", "daughterplus_PT","daughterplus_IP_OWNPV"]
    testData        = pd.read_csv(f"data/{mode}/TestData.csv",  index_col=0)
    trainData       = pd.read_csv(f"data/{mode}/TrainData.csv", index_col=0)

    hiddenLayers    =  [Dense(len(inputVariables),    input_shape = (len(inputVariables),),    activation="relu",    kernel_initializer="random_normal"),
                        Dropout(0.0),
                        Dense(len(inputVariables)*12, input_shape = (len(inputVariables),),    activation="relu",    kernel_initializer="random_normal"),
                        Dropout(0.0),
                        Dense(1,                      input_shape = (len(inputVariables)*12,),    activation="sigmoid", kernel_initializer="random_normal")]

    model = NeuralNetworkClassifier(hiddenLayers, name, inputVariables, trainData, testData)
    model.createInFull()


def createFoldedDataModels(nfolds = 5, mode = "pipi"):

    inputVariables  = ["nTracks", "B_P", "B_Cone3_B_ptasy", "B_ETA", "B_MINIPCHI2", "B_SmallestDeltaChi2OneTrack", "B_FD_OWNPV", "piminus_PT", "piminus_IP_OWNPV","daughter_neutral_PT", "daughter_neutral_IP_OWNPV", "daughterplus_PT","daughterplus_IP_OWNPV"]
    FullData        = pd.read_csv(f"data/{mode}/FullData.csv",  index_col=0)
    FullData        = FullData[inputVariables+["isSignal", "weights"]]
    FullData        = np.array(FullData)
    
    kfold           = KFold(nfolds)

    FoldedTestData  = []
    FoldedTrainData = []

    for train, test in kfold.split(FullData):
        FoldedTrainData.append(pd.DataFrame(FullData[train], columns=inputVariables + ["isSignal", "weights"]))
        FoldedTestData.append( pd.DataFrame(FullData[test],  columns=inputVariables + ["isSignal", "weights"]))

    fold = 0
    for (trainData, testData) in zip(FoldedTrainData,FoldedTestData):
        
        print(trainData)

        fold     += 1
        name      = f"fold{fold}{mode}"

        GlobalParams    = { "name"           : name,
                            "inputVariables" : inputVariables,
                            "testData"       : testData,
                            "trainData"      : trainData}

        hiddenLayers    =  [BatchNormalization(axis = 1),
                            Dense(len(inputVariables),    input_shape = (len(inputVariables),),    activation="relu",    kernel_initializer="random_normal"),
                            Dropout(0.05),
                            BatchNormalization(axis = 1),
                            Dense(len(inputVariables)*12, input_shape = (len(inputVariables),),    activation="relu",    kernel_initializer="random_normal"),
                            Dropout(0.05),
                            BatchNormalization(axis = 1),
                            Dense(len(inputVariables),    input_shape = (len(inputVariables)*12,), activation="relu",    kernel_initializer="random_normal"),
                            Dropout(0.05),
                            BatchNormalization(axis = 1),
                            Dense(1,                      input_shape = (len(inputVariables),),    activation="sigmoid", kernel_initializer="random_normal")]

        print(f"\n-- fold {fold} \n")
        print("- Random Forest")
        RF = ForestClassifier(RandomForestClassifier(n_estimators=1500, max_depth=6, verbose=1), **GlobalParams)
        RF.createInFull(path="savedModels/foldedModels")
        
        print("- AdaBoost")
        AD = ForestClassifier(AdaBoostClassifier(n_estimators=500, learning_rate=0.1), **GlobalParams)
        AD.createInFull(path="savedModels/foldedModels")

        print("- Neural Network")
        NN = NeuralNetworkClassifier(hiddenLayers, **GlobalParams)
        NN.createInFull(path="savedModels/foldedModels")

        print("- Gradient Boost")
        GB = ForestClassifier(GradientBoostingClassifier(n_estimators=800, learning_rate=0.012, max_depth=4, verbose=1), **GlobalParams)
        GB.createInFull(path="savedModels/foldedModels")

